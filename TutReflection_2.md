# Tutorial Reflections
_Feb 6, 2016_

## Wget _(The 'W' stands for "with great power comes great responsibility")_

The Programming Historian tutorials about [data-mining](http://programminghistorian.org/lessons/data-mining-the-internet-archive) and working with [Wget](http://programminghistorian.org/lessons/automated-downloading-with-wget) touched on some of the points that I made in my previous reflection. As one of my peers pointed out GNU Wget is a free software and GNU is sponsered by The Free Software Foundation [(FSF)](http://www.fsf.org/).
>"Our mission is to preserve, protect and promote the freedom to use, study, copy, modify, and redistribute computer software, and to defend the rights of Free Software users" - FSF

Wget is a commandline tool that makes it possible to download the files and code that make up a website. Wget makes it very easy to obtain a mirror copy of a webpage or of an entire website. There are similar tools out there that can be used to automatically scrap or download large amounts of data and information from websites and digital databases. The simplicity of this tool makes it very easy to use and understand, this also means that there are those who have found ways to block Wget from being able to access and copy certain parts of their site. This can be done with a [_Robot Exclusion Protocol_](http://www.robotstxt.org/robotstxt.html), which is written in to a robot.txt file that sites in the backend of a website. Very similar to the `oncontextmenu="return false;"` or `ondragstart="return false" onselectstart="return false"` of _yesteryear_, which would disable a visitor's ability to right-click and save an image or their ability to select and copy/paste text on a webpage.

There is something genuinely frustrating about a digital database that is free to access and search through but uses a robot.txt file to impair a user's ability to obtain the data for their own use. Mind you, I understand the value of a robot protocol when a tool like Wget can duplicate not only the data but all bits and bobs that are unique to only to one site - as is the case with [Witching.org](http://witching.org/), whose interactive visual interface, [Throwing Bones](http://witching.org/throwing-bones/) and [Brimestone](http://witching.org/brimstone/) were created for this digital humanities project.

>"...this study will create visual meaning from a core group of texts published in Early Modern England between 1500-1700, written by a series of authors with competing agendas. Rather than strip-mining the texts for their hidden value, it will unfold them, letting the texts, as well as the researcher, create and recreate meaningful visual narratives." - [WEME](http://witching.org/content/scope-and-objectives)

>"All of these resources are designed to compliment one another. WEME is creating a visual schema to compliment the conceptual schema used to categorize witchcraft. Representations of events and people are colour coded and illustrated. The legend on Throwing Bones will show you cards and their classifications." - [WEME](http://witching.org/content/tutorial)

The collection of data that was brought together and collated for this project would be a valuable resource for researchers. Which is why I am perplexed by the fact that the project does not provide a way to access or retrieve a stripped down version of the database. The University of Edinburgh created the [Survey of Scottish Witchcraft](http://webdb.ucs.ed.ac.uk/witches/) Database containing nearly 4000 records of accused witches and related materials. The witch trials in Scotland had a much greater impact on history than the trials in England, and it has made the data accessible for the free use of anyone interested. 
